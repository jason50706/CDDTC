{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import evaluate\n",
    "from args import args, config\n",
    "from items_dataset import items_dataset\n",
    "from models import Model_Crf, Model_Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader  = torch.load(args.DATA_PATH + \"test.pt\")\n",
    "#test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, collate_fn=test_set.test_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../model/\"\n",
    "model_name = \"roberta_crf_epoch_1.pt\"\n",
    "device = torch.device('cuda', 0) if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "if args.use_crf:\n",
    "    model = Model_Crf(config).to(device)\n",
    "else:\n",
    "    model = Model_Softmax(config).to(device)\n",
    "model.load_state_dict(state_dict=torch.load(directory + model_name, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list, ground_truth, text_list = evaluate.test_encode_predict(test_loader, device, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"Charecter based scores for check_worthy:\")\n",
    "all_confusion_matrix = dict()\n",
    "all_scores = dict()\n",
    "for metric in ground_truth_dict.keys():\n",
    "  confusion_matrix = evaluate.cal_span_level_matrix(predict_list, ground_truth_dict[metric], text_list)\n",
    "  scores = evaluate.calculate_precision_recall_f1(confusion_matrix)\n",
    "  all_confusion_matrix[metric] = confusion_matrix\n",
    "  all_scores[metric] = scores\n",
    "  print(metric)\n",
    "  print(confusion_matrix)\n",
    "  print(scores, end =\"\\n\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Charecter based scores for majority vote:\"\n",
    "confusion_matrix = evaluate.cal_span_level_matrix(predict_list, ground_truth, text_list)\n",
    "print(confusion_matrix)\n",
    "#print(\"check_worthy\", evaluate.calculate_precision_recall_f1(confusion_matrix))\n",
    "check_worthy = evaluate.calculate_precision_recall_f1(confusion_matrix)\n",
    "print(\"check_worthy\", check_worthy)\n",
    "\n",
    "confusion_matrix_non_worthy = confusion_matrix.copy()\n",
    "tmp = confusion_matrix_non_worthy[\"TP\"]\n",
    "confusion_matrix_non_worthy[\"TP\"] = confusion_matrix_non_worthy[\"TN\"]\n",
    "confusion_matrix_non_worthy[\"TN\"] = tmp\n",
    "tmp = confusion_matrix_non_worthy[\"FN\"]\n",
    "confusion_matrix_non_worthy[\"FN\"] = confusion_matrix_non_worthy[\"FP\"]\n",
    "confusion_matrix_non_worthy[\"FP\"] = tmp\n",
    "non_check_worthy = evaluate.calculate_precision_recall_f1(confusion_matrix_non_worthy)\n",
    "print(\"non_check_worthy\", non_check_worthy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"macro F1: %.4f\" %((check_worthy[\"f1\"]+non_check_worthy [\"f1\"])/2))\n",
    "print(\"macro Precision: %.4f\" %((check_worthy[\"precision\"]+non_check_worthy [\"precision\"])/2))\n",
    "print(\"macro Recall: %.4f\" %((check_worthy[\"recall\"]+non_check_worthy [\"recall\"])/2))\n",
    "\n",
    "print(\"    Checkworthy, Non_check_worthy\")\n",
    "print(\"F1: %.4f,         %.4f\" %(check_worthy[\"f1\"], non_check_worthy [\"f1\"]))\n",
    "print(\"Precision: %.4f,         %.4f\" %(check_worthy[\"precision\"], non_check_worthy [\"precision\"]))\n",
    "print(\"Recall: %.4f,         %.4f\" %(check_worthy[\"recall\"], non_check_worthy [\"recall\"]))\n",
    "\n",
    "print(\"Acc: %.4f\" %check_worthy[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d6017e34087523a14d1e41a3fef2927de5697dc5dbb9b7906df99909cc5c8a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
